[
  {
    "objectID": "posts/ethical-use-algorithms-and-data/index.html",
    "href": "posts/ethical-use-algorithms-and-data/index.html",
    "title": "Ethical use of algorithms with data",
    "section": "",
    "text": "Preface\nBefore I talk about my views on ethics in the realm of Data Science, I first want to talk about how I got into Data Science. I spent the first two years of my career doing some analysis accompanied by mostly Data Engineering before I knew it was Data Engineering. At this two year mark in 2014, I was at a point where I wanted to figure out where I wanted to take my career next. After a bit of searching, I landed on the field of Data Science since it seemed like a perfect fit with my love of statistics and working with data. Like with anything, my first approach was to search and try and find as much information on the topic as possible. I took a few Coursera courses, followed top data scientists on twitter, read blogs, and listened to podcasts. More often than not, the topics were usually around applications of machine learning, interesting papers on machine learning models, or interviews with industry experts. However, periodically there was a post or podcast that would catch my eye on a topic like “gender biased word embeddings.” I quickly realized that although Data Science has the propensity for good, it also has a potential to harm individuals or entire communities. I began actively searching for these issues and came upon a couple of good articles that covered a wide variety of issues such as Biased Algorithms Are Everywhere, and No One Seems to Care\nHowever, before I go any further, I would like to introduce two concepts that are at the center of this post.\n\nMachine Learning: “Machine learning algorithms build a mathematical model of sample data, known as”training data”, in order to make predictions or decisions without being explicitly programmed to perform the task.” - https://en.wikipedia.org/wiki/Machine_learning\nAlgorithmic Bias: “Algorithmic bias occurs when a computer system reflects the implicit values of the humans who are involved in coding, collecting, selecting, or using data to train the algorithm.” - https://en.wikipedia.org/wiki/Algorithmic_bias\n\nWith that covered, I will move on to discuss a handful of cases where models are either biased, unethically created, or unethically used.\n\n\nGender biased word embeddings\nBefore I talk about how a word embedding can be gender biased, first I would like to discuss what is a word embedding. A word embedding is a useful natural language processing tool where a model can represent the relationships between words as mathematical values to allow for associations such as “man:king” with “woman:queen” and “paris:france” with “tokyo:japan”. Cool, right? However, this bias has been extended to “man:programmer” with “woman:homemaker”. Not cool. A common model used is Word2Vec developed by Google back in 2013 trained on Google News texts. Unfortunately, because the data this model was trained on was gender biased, so are the results. But there is hope! Researchers have done work to both quantify the bias and come up with methods to “debias” the embeddings in Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. For more on this, I recommend How Vector Space Mathematics Reveals the Hidden Sexism in Language.\n\n\nMortgage loan interest rate bias\nI find this to be a more traditional instance where financial institutions set out to use “big data” with “machine learning” to find ways to infer interest rates based on geography or characteristics of applicants. This is referred to as “Algorithmic Strategic Pricing”. A result of this, based on a study done by UC Berkly, African Americans and Latino borrowers pay more on purchase and refinance loans than White and Asian ethnicity borrowers. “The lenders may not be specifically targeting minorities in their pricing schemes, but by profiling non-shopping applicants they end up targeting them” said Adair Morse.\nFor more information, you can check out Minority homebuyers face widespread statistical lending discrimination, study finds or the study itself Consumer-Lending Discrimination in the Era of FinTech\n\n\nImage recognition bias\nImage recognition has become an everyday utility in society with many big tech companies using it in their product offerings like Google, Microsoft, and most notably, Facebook. However, without any industry benchmarks to ensure that these facial recognition applications perform well on people of all races, genders, and age, there are instances where either the systems simply do not work or the systems are offensive.\nA more notable instance of a facial recognition system failing to work was discovered by Joy Buolamwini, who is an African American PhD student at MIT’s Center for Civic Media. At the time of her discovery, she was a Computer Science undergraduate at Georgia Tech. In her undergrad, she was working on a research project to teach a computer to play “peek-a-boo” and found out that although the system had no issues recognizing her lighter skinned roommates, it had difficulty working with her. Her solution to this was to wear a white Halloween mask which would then detect her as white. More on this can be found here Ghosts in the Machine\nThis issue is not limited to Joy Buolamwini’s research, but could also be seen in Microsoft’s Kinect for the X-box. Back in 2010, it was observed that Microsoft’s Kinect often times would not work on people with darker skin. Is Microsoft’s Kinect Racist?. However, additionally worth noting is Microsoft advocating for there to be more regulation around image recognition in their blog post Facial recognition technology: The need for public regulation and corporate responsibility.\n\n\nMicrosoft Tay twitter bot\nI like using Microsoft’s Tay twitter bot as an example regarding ethics since it is an instance where the researchers themselves weren’t being unethical, but failed to consider how their model could be interacted with and manipulated. To provide a brief summary, Microsoft’s Tay twitter bot was a research experiment conducted where they built an artificial intelligence twitter bot that was supposed to learn how to mimic the speech of a 19 year old American girl by interacting with people on twitter. However, what they failed to consider was a series of internet users deciding to bombard the twitter bot with hateful speech. The end result required Microsoft to turn the twitter bot off after 16 hours.\nThis is an instance where although the researchers themselves were not being unethical, they failed to take into consideration how their twitter bot could be manipulated. As we build products, it is important not only to think about what the purpose of the model is but how it could be used to harm other people.\n\n\nFacebook Cambridge Analytica scandal\nAn ethics blog post would be incomplete without mentioning the Facebook–Cambridge Analytica data scandal. To briefly summarize, this is an instance where an organization used a survey app through Facebook to collect information from users for supposedly academic purposes. However, through manipulating Facebook’s app design, they were also able to collect the information of not only the users that agreed to the survey, but all of the users’ friends information as well. Furthermore, instead of using this information for academic purposes, they used it for both the Ted Cruz and Donald Trump political campaigns.\nThe two main takeaways here were that Cambridge Analytica both collected people’s information without their consent and then used the information for purposes beyond the consent given. Needless to say, collecting people’s information without their consent is clearly unethical. However, even when collecting people’s personal information ethically , it is important that measures are taken to ensure their information is protected and not misused.\n\n\nWays to improve\nA few closing thoughts on ways the Data Science industry can improve:\n\nBuild teams of people from diverse backgrounds to ensure underrepresented communities are not negatively impacted by biased models.\nAudit algorithms AND the data sets used to train models.\nEncourage companies to provide more information to users and researchers to help them better understand potential pitfalls and biases that may exist in their tools.\n\n\n\nAdditional sources\nIf you are still interested in looking more into this topic, I highly recommend checking out the following:\n\nArticles\n\n“We’re in a diversity crisis”: cofounder of Black in AI on what’s poisoning algorithms in our lives\nMachine bias risk assessments in criminal sentencing\n\nPodcasts\n\nData Skeptic - Data Ethics\nLinear Digressions - Facial recognition, society, and the law\nLinear Digressions - When is open data too open?\n\nBooks\n\nWeapons of Math Destruction"
  },
  {
    "objectID": "posts/different-roles-in-data/index.html",
    "href": "posts/different-roles-in-data/index.html",
    "title": "What are the different roles in data?",
    "section": "",
    "text": "Preface\nThe intended audience for this post are people interested in getting into data or businesses looking to use data to drive business value. With this in mind, I attempted to stay high level and provide enough information and key words for someone to find more in-depth resources on any given topic. Additionally, when highlighting skills, I focus on what I have observed being commonly used in industry based on experience, combing through job postings, and networking.\n\n\nWorking in data\nIn my experience, working in data can at times be a bit confusing and overwhelming because of the sheer breadth of skills and techniques that are needed to process, manage, and analyze data. In this post, I am going to group these sets of skills into different roles, highlight the order in which they should be built out in an organization, and emphasize the importance of only focusing on one role at a time. As someone who has done work in each of these areas, my effectiveness was significantly reduced when I had to wear multiple hats.The four roles I will be focusing on in this post are Data Analyst, Data Engineer, Data Scientist, and Machine Learning Engineer. While some of these roles can be broken down to be more specific, my goal of this blog post is to stay high level and emphasize broad concepts around each role. Additionally, there are other important roles such as Data Governance and Privacy and Data Curation which I will not be covering as this post is meant to be an introduction to a data team in an organization.\nAnd one last thing, although I do not advise trying to perform multiple roles at once, I think it can be beneficial if given the opportunity to try different roles and see what you enjoy most.\n\n\nThe different roles\n\n\nThe Data Analyst - Describes Data\nIn my opinion, the Data Analyst is at the heart of a data team. Their focus should be to describe data to help drive business value.\nA Data Analyst should be the first role a company should hire and the expectations should be for them to come in and work with stakeholders and leadership to understand a company’s goals and business problems and how data can help drive value. Once a company’s goals and business problems have been clearly defined, the Data Analyst should seek out data, wrangle it to make it tidy (see Tidy Data by Hadley Wickham), and then deliver reports and/or dashboards to communicate the results. The Data Analyst is what I consider more of an operational role that helps a business measure key indicators to help understand the state of their business as well as make data driven decisions.\nKey areas: data wrangling - data exploration - business intelligence - building dashboards (Key Performance Indicators (KPI), Operational) using tools like Tableau, PowerBI, Looker - building reports using tools like SQL Server Reporting Services (SSRS) - SQL - (optional but recommended) R or Python\n\n\nThe Data Engineer - Scales Data\nThe Data Engineer’s role is to scale an organization’s data ingestion and management.\nOnce a company has a firm understanding of their goals and business problems where data can help drive value, a Data Engineer can come in scale and build upon existing data processes. By scale, I mean the 3Vs of data(volume, variety, and velocity) and the ability to build automated data pipelines capable of bringing in data at the frequency a business needs to support applications, reports, dashboards, and analyses. Not only should a Data Engineer focus on Extract, transform, load (ETL) or Extract, load, transform (ELT), they should also have a good understanding of how to build optimized data warehouses/marts/stores to be useful for end users. Lastly, a Data Engineer should understand the concept of building production code, and by that I mean it needs to work consistently and reliably. In the event of an inevitable failure, it needs to fail reliably and have adequate logging so the issue can be debugged and resolved as efficiently as possible.\nKey Areas: concept of production code - optimized data warehousing/mart/store - Extract, transform, load (ETL) or Extract, load, transform (ELT) - data pipelines - automation - streaming data - web scraping - tools like Talend or Informatica - SQL - Python - docker\n—————————————————————\nMost companies can be highly successful doing Data Analytics and Engineering well. Without these two areas in place, a Data Scientist will be ineffective since they will simply be doing a combination of Data Analytics/Engineering. Often times this leads to mismatched skill sets and expectations on both the side of the organization as well as the Data Scientist.\n—————————————————————\n\n\nThe Data Scientist - Models Data\nThe role of the Data Scientist is to use machine learning and statistical techniques to develop models of data capable of predictions or prescriptions.\nThese models should have a focus on enhancing a business’ existing processes or develop new products that are not as feasible without machine learning techniques. This is more of a strategic role where often times the goal is to build upon the existing Data Analytic work. Similar to that of a Data Analyst, a Data Scientist will need to do data exploration and visualization to gain a better understanding of the data. Additionally, they will need to interact with business stakeholders to gain a firm understanding of company goals and business problems to help guide what models can be built that can drive value.\nKey Areas: feature engineering - statistical analysis - building / tuning / evaluating machine learning models - natural language processing - computer vision - R or Python - SQL\n\n\nThe Machine Learning Engineer - Scale Models\nThe role of the Machine Learning Engineer is to scale models by deploying and managing them in a production environment.\nA Machine Learning Engineer should work closely with the Data Science, Data Engineering, and Software Development teams to understand how a model needs to be deployed and build out necessary tools, APIs, or batch processes. Additionally, a Machine Learning Engineer needs to think about putting in place tools that will evaluate model performance and look for model drift to evaluate if a model needs to be retrained (great blog about this topic The Ultimate Guide to Model Retraining). Like a Data Engineer, they need to have a concept of production code since once a model is deployed, it is often their responsibility to ensure predictions are not only being returned but that they meet the necessary expectations.\nKey Areas: model drift - model retraining - the concept of production code - evaluation of models in production - machine learning - feature engineering - Python - SQL - docker - automation\n\n\n\nOverall Suggestions when working in Data\n\nFocus on the business problem - When starting data projects, it is easy to get distracted by all of the different available technologies. However, it is important to first focus on solving the business problem at hand versus using current “state of the art” solutions.\nBe intentional with technology selection - Whenever introducing a new piece of technology or programming language to a company, it is crucial to consider the consequences. Adding new technologies increases the overall complexity of a company’s technology stack as well as often times reducing collaboration and consumption of work across teams.\nSource Control - Regardless of role, I highly recommend all code (including SQL) is managed in a source control platform. By managing code through a source control platform, you can ensure code is accessible and changes to it over time can be tracked and managed. Depending on your team’s maturity and needs, I suggest checking out either Trunk Based or a Gitflow Development Strategy.\n\nSome popular source control platforms are GitHub, GitLab, and BitBucket\nIn instances where Data Analysts or Data Scientists are doing analyses or ad-hoc requests, I recommend having a single repository where the code for these requests are stored and managed with loose processes around merging. This will help ensure the code is located in a more discoverable and central location. With that, a more restrictive repository(s) should be in place that requires reviews for managing code related to specific project work.\n\nCloud computing is becoming more common place when working with data in organizations. AWS, Microsoft Azure, and Google all offer free tiers that can allow you to get some hands on experience even if your organization does not use cloud computing.\nPackage management - With respect to R and Python, I suggest looking into utilities to manage packages to help with reproducibility.\n\nFor R you I suggest using renv over packrat as it is much more efficient at managing packages.\nFor Python, I suggest using venv. It is simple and I have never had issues using it yet.\nIf you are new to Python, I also recommend checking out Anaconda. However, I will admit my experiences with using it to manage and share environments has not been great. I only recommend Anaconda as it allows you to more easily get started in Python.\n\nDocker - While managing packages is helpful, docker allows you to manage the entire environment of a script which ensures reproducibility. Dev Ops people will love you.\n\nA good resource for doing this in R is An Introduction to Docker for R Users by Colin Fay\nA good resource for Python is Docker\n\n\n\n\nUpdates\n\n2019-12-08\n\nrenv is now offered on cran, yay!\nChanged the method of suggest python environment management to being venv based on research and testing for consistently and reliably setting up and management environments in python."
  },
  {
    "objectID": "posts/the-process-of-data/index.html",
    "href": "posts/the-process-of-data/index.html",
    "title": "The Process of Data",
    "section": "",
    "text": "Do you have complex processes that you are struggling to streamline or automate? If so, my recommended approach has some key benefits:\n⭐ Shorter timelines for improving an overall process\n⭐ Faster ROI when investing in technology\n⭐ Purpose built tools that are less expensive and easier to use\n⭐ Improved data quality\n⭐ Improved data accessibility and ease of use\n🙈 Problem: Building tools or platforms that streamline an entire or multiple parts of a process can be complex to build, complex to use, and expensive. This can also prevent you from being able to use existing purpose built tools since any one tool may only be able to streamline a portion of your process. 🙈\n💡 My recommended steps are: 💡\n1️⃣ Document the whole process. This could include an overall process flow along with procedural steps for each part. This step is the most important since it is challenging to streamline an undocumented process.\n2️⃣ Break the process down into separate parts. When a process is broken down into separate parts, it allows for less complex solutions to be used to automate it versus attempting to streamline an entire process with one tool.\n3️⃣ Identify one part of the process that is the most important and/or time consuming. Rarely are all parts in a process made equal. Usually there are a handful of parts that are either crucial to success and/or time consuming.\n4️⃣ Automate or Streamline Automate or streamline the part of the process identified in 3️⃣. This could involve using an existing tool or building a new tool. ⚠ If a tool needs to be built, it is important to keep in mind this new tool is only meant to solve a single part of the overall process. ⚠\n5️⃣ Iterate. Start back over at 1️⃣ Once you have automated or streamlined the most important and/or time consuming part of a process, it is helpful to go through the whole exercise again. This will help inform what worked, what didn’t work, and things to keep in mind when continuing to streamline a process.\nSome final notes:\n🗒 Key people that do the process should be involved. There are always nuances to a process that can be missed in documentation.\n🗒 It is helpful to try and keep an overall technology architecture in mind. Selecting tools that can’t work with one another leads to data silos and forced manual parts of a process.\n🗒 Have a high level understanding of what an ideal end state/process looks like. This is helpful in thinking beyond the current process and can help inform technologies to use."
  },
  {
    "objectID": "posts/venv-is-a-data-professionals-best-friend/index.html",
    "href": "posts/venv-is-a-data-professionals-best-friend/index.html",
    "title": "venv is a Data Professional’s Best Friend",
    "section": "",
    "text": "Preface\nHave you ever tried to share a project with a colleague and they struggled to run it? Have you ever spent hours trying to get a machine learning library to work? If so, there are solutions out there that can help with this! In this post, I will talk about a utility, virtual environments, that can be used to manage specific packages.\n\n\nvenv/renv\nWhen starting up a project, the first step I take is to spin up a virtual environment. In python, my personal favorite is the virtual environment utility built directly into python, python venv. In R, I like to use the package renv supported by RStudio. I do this for the following reasons:\n\nClean slate: When installing packages in a virtual environment, you can either use the latest versions of packages without worrying about impacting other projects or choose to use specific versions of packages.\nCollaboration: If I am working on a project with others, I can share the virtual environment configuration files, and they can spin up a virtual environment and run my code with the same packages.\nReproducibility: When I set the project down and I want to pick it up at a later time, I can ensure I am using the same packages as when I first started the project.\n\nFor python, you can run:\ncd /path/to/project\n# creates the virtual environment\npython -m venv venv\n# activates the virtual environment in your terminal\nsource venv/bin/activate\nOnce the environment is created and activated, you can then install all of the packages you need for your project and save them to a requirements.txt file that should be included with your project when sharing.\npip freeze > requirements.txt\nFor R, you can run:\n# You must first install `renv`\nRscript -e \"install.packages('renv')\"\n\n# Once installed, you can then run\ncd /path/to/project\nRscript -e \"renv::init()\"\nOnce set up and activated, you can start working on your project and install packages as needed. When you are ready to snapshot your environment, you can run the following command:\nRscript -e \"renv::snapshot()\"\nThis will update the renv.lock file that should be used when sharing your code to allow setting up a new environment from it. This is most easily used when working with R in a project in RStudio.\n\n\nLimitations\nWhile virtual environments are great at managing packages needed to run code, it is limited to just that. There are other aspects of running a project such as application or operating system dependencies. To completely solve for reproducing the environment for running code, you can explore using docker, a utility that will allow for capturing your entire environment which includes the Operating System and supporting applications. There is a steeper learning curve to this, and I plan on dedicating entire post(s) to the topic of how docker is a more comprehensive solution to reproducibility.\n\n\nOther virtual environment tools\nFor python, pipenv and poetry are virtual environment utilities that allow for managing various versions of packages in a project. You can manage packages ranging from selecting a specific version of a package to letting any version of that package be installed. While these tools can be helpful in managing packages and package dependencies, they come with additional complexity and a learning curve."
  },
  {
    "objectID": "posts/tool-selection-framework/index.html",
    "href": "posts/tool-selection-framework/index.html",
    "title": "Tool Selection Framework",
    "section": "",
    "text": "Overview\nThe goal of the Data Tools Conceptual Framework is to highlight various tools that can be used to process and analyze data and data storytelling. While most tools can be used to accomplish a task, there are instances where some tools may be better suited to complete a given project.\nTo distill down a project down into something that can be evaluated, a project can be broken down into individual work units. A project will be evaluated based on the volume and complexity of work units. For projects that have either a combination of complex work units or a large volume of work units will notably impact the type of tools that would be better suited.\nThe key metrics that will be used when evaluating data tools will be the learning curve and how productive a person can be with a tool. While some tools may have a lower learning curve making them easier to pick up, if a project is large and/or complex, a tool that allows a person to be more productive could allow for the project to be done faster and with higher quality.\n\n\nDefining a Project\nA project consists of one or many work units. A work unit is a loose concept meant to represent the steps taken to complete a part of a project. When evaluating work units, it is important to keep in mind the volume and complexity of work units for a project.\n\nComplexity\nThe complexity of a work unit is a function of how many steps there are in a single work unit as well as if there are nuances in a work unit based on the input. Work units that have higher complexity will result in each individual unit taking longer to complete and increasing the opportunity for human error.\n\n\nVolume\nThe volume of work units in a project is how many items need to be processed. This could range from either working with a single data file downloaded from a data platform or having to process three hundred sensor data files. Volume is important to keep in mind since even for projects where the individual work complexity is low, if there is a significant volume of units it could lead to the overall project taking a long time and increasing the likelihood of errors.\n\n\n\nData Tool Metrics\nWhen looking at data tools, a comparison will be made between the learning curve of the tool and how productive a person can be with that tool.\n\nLearning Curve\nThe learning curve represents how long it will take to initially learn the tool.\nFor scoring, a high score represents a lower learning curve, and a low score represents a higher learning curve.\n\n\nProductivity\nProductivity is determined by how efficiently a person can complete an individual work unit. This is important since there could be instances where a person may have a large volume of work units for a given project or they may have to repeat a work unit because either an error was discovered or a step was missed.\nFor scoring, a high score represents a tool that allows a person to be very productive with the tool versus a low score meaning the tool is more manual.\n\n\n\nData Tools\nData Tools will be evaluated from the following categories: Spreadsheet, Programming Language, and Data Storytelling. An overview will be provided for where the tool excels and struggles in relation to the score.\n\nSpreadsheet\n\nOverview\n\n\n\nLearning Curve\nProductivity\n\n\n10\n1\n\n\n\nExcel and Google Sheets are both good examples of a spreadsheet tool. What is great about Excel and Google Sheets is that both have a low learning curve to get started and over the years have added many features that make it easy to go from structured data to insights. This could involve creating a pivot table that allows for easily aggregating and summarizing data to creating a variety of pre-baked charts that allow for a decent amount of customization.\nHowever, with the lower learning curve also comes with low Productivity. Since spreadsheet tools only allow manual input, each work unit must be done manually. This can become an issue with projects that have a large amount of work units or if a project has work units with high complexity.\nOne caveat is that there is the ability to write macros in a spreadsheet tool to enhance the functionality but at that point, it would be more beneficial to take a step back to evaluate whether a Data Preparation Tool or Programming Language would be better suited for the project.\n\n\nCapabilities\n\nData Preparation\nData Storytelling\n\n\n\nIdeal Project\nWhere Excel and Google Sheets excel are when you have a project that has a low volume of work units, and the complexity of the work units is low. In this scenario, the amount of time it takes to complete the project is low and additional time can be spent ensuring the quality of the project.\n\n\nCost and Availability\nWhere Google Sheets is free with a Google Account, Microsoft Excel has to be purchased if you are not in a setting where you are working at a company or are in school where Excel is provided. Additionally, Microsoft Excel works best on a Windows PC where Google Sheets is browser based and works in any operating System.\n\n\n\nData Visualization Tools\n\nOverview\n\n\n\nLearning Curve\nProductivity\n\n\n6\n2\n\n\n\nData Visualization Tools such as Tableau, Juicebox, and PowerBI allow for creating Data Visualizations with structured and cleansed data. Data Visualization Tool’s will allow for being more productive than using a Spreadsheet tool because of the additional features.\n\n\nCapabilities\n\nData Visualization\n\n\n\nIdeal Project\nData Visualization tools will excel at instances where there is a project where data has already been prepared. This will work better with low work units and lower complexity. However, because it has more features, it allows for some additional complexity. Will still struggle with higher volume of work.\n\n\nCost and Availability\nData Storytelling Tools must be purchased. This makes them more costly and less accessible.\n\n\n\nProgramming Language\n\nOverview\nWhere Programming Languages like R and Python have the greatest learning curve, they also allow for a person to be the most productive and makes it easier to produce higher quality end products. Additionally, as R and Python are both open source, that makes them free and very accessible.\nOne of the greatest challenges to getting started with R and Python has been setting them up on a computer. However, there are free offerings for both R and Python allowing for them to be run in a browser without the need for setting up a personal computer.\n\n\nCapabilities\n\nData Preparation\nData Visualization\n\n\n\nIdeal Project\nProgramming Languages have packages that support a wide variety of project types. However, taking into consideration the learning curve and the amount of time it takes to set up a project, an ideal projects for Programming Languages are ones that require reproducibility, processing a large amount of items, and have complex steps.\n\n\nCost and Accessibility\nThere is no cost to using R and Python. It is accessible for anyone to use that has access to a computer with internet.\n\n\nOverall Scoring\nBelow is a list of scores for several types of tools.\n\n\n\n\n\n\n\n\nTool\nLearning Curve\nProductivity\n\n\nSpreadsheet Tool (Excel, Google Sheets)\n10\n1\n\n\nData Visualization (Tableau, Juicebox, PowerBI)\n6\n5\n\n\nProgramming Language (Python, R)\n1\n10\n\n\n\n\n\n\n\nConclusion\nFor projects that have a low volume of work with low complexity, a Spreadsheet Tool will be sufficient. However, if either the complexity or volume of work is high, over the lifetime of a project it would be beneficial to explore using a combination of programming languages and data visualization tools."
  },
  {
    "objectID": "posts/impactful-data-over-big-data/index.html",
    "href": "posts/impactful-data-over-big-data/index.html",
    "title": "Impactful Data > Big Data",
    "section": "",
    "text": "Preface\nIn the world of data, data is categorized as either “big data” or just “data.” This can be problematic since when an organization doesn’t have “big data,” more often than not they do not take time to automate processing their data nor take care in efficiently managing it. On the other hand, an organization may have what they consider “big data” and they spend a significant amount of effort and money managing it, even if there is no clear use-case. Because of this, I think the focus should shift from “big data” to “impactful data.”\n\n\nWhat is data?\nBefore explaining what impactful data is, a quick definition about what is data. As a Data Engineer, to me data is a collection of values to represent a transaction, measurement, or an event along with information to provide context. An example of this could be data coming off sensors placed in a river. The sensor is set to collect measuremetns of pH, Temperature, and Dissolved Oxygen. Each measurement has a time stamp associated with it along with information about when and where the sensor was placed, when it was collected, quality check information about the sensor, etc. For a more detailed definition of data, Wikipedia has a good article around it.\n\n\nWhat is impactful data?\nTo simply put it, impactful data has a clear use-case that can help solve a problem and/or answer a question that will help inform a decision. This could range from being used to create a KPI metric dashboard, data collected in a scientific study, to data that can help drive Government policy. In the absence of a clear use-case, my recommendation would be to document the data and archive it in cheaper object storage, such as AWS S3.\n\n\nShould you automate processing and management of small volumes of impactful data?\nYes! Regardless of the volume of the impactful data, taking the time to streamline processing and managing the data can have a wide variety of benefits.\n\nAutomating Data Processing\nAutomating data processing reduces the time it takes to gain access to the data and insights from it. It also reduces the amount of potential human error when manually processing data. This could start as a simple Python or R script run locally. As the volume and frequency of data increases, this could evolve into building a Serverless Data Pipeline in AWS using services like AWS Lambda, AWS Step Functions, and/or AWS Fargate. Depending on situation, there are pros and cons between running locally and deploying a data pipeline in a cloud platform. Regardless of the implementation, automating data processing leads to faster access to the data and insights along with less human errors.\n\n\nData Management\nWhen does it make sense to build a data warehouse and marts? Well…“it depends.” If you are pulling in data from multiple desperate sources that you need to normalize together into a single harmonized data model, then a data warehouse and curated data marts may be appropriate. If the data is coming from a single source, it may be sufficient to streamline processing it and serving the results up via a Quicksight Dashboard.\n\n\n\nWhen does volume of data matter?\nThere are some cases where the volume data will matter such as when evaluating a data platform architecture or exploring using Machine Learning to solve a problem.\n\nEvaluating Data Platform Architecture\nWhen building a data platform, it is important to take into consideration the 3 V’s of data (volume, velocity, and variety). This will inform the kinds of technologies a Data Engineer should consider using in order to ensure the data stack can handle the use-case. An example being a data platform to manage terabytes of data, a Data Engineer should consider frameworks that support parallel computing such as Spark with AWS EMR versus if I am managing gigabytes of data where I could use something simpler to use such Lambda functions to process the data and AWS Athena to query the data.\n\n\nMachine Learning Applications\nWhen Machine Learning came onto the scene, it brought with it the need for large volumes of data in order to have sufficient data to train an algorithm to perform a task such as image classification, natural language processing, etc. In these cases, the larger the training dataset, more often than not the better the trained algorithm will perform. This is of course assuming the training dataset is well suited for the problem to be solved but that would deserve it’s own blog post.\n\n\n\nConclusion\nIt is important to think critically about the actual value of a dataset over the perceived value. Also, any dataset that is considered valuable, there are numerous benefits to invest time and energy into automating the processing and management of the data. This will help improve data quality and ensure data is accessible in a timely manner."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Project Name\nDescription\nLink\n\n\n\n\nProcessMassSpecData\nWorking with a research lab at the University of Florida to streamline calculating the concentration of analytes in samples from Mass spectrometry data.\nProcessMassSpecData"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Antonison's Blog",
    "section": "",
    "text": "Hello, I’m Alex Antonison. I’m a Data Engineer with 10+ years of experience performing in Data Engineering roles across Government, Energy, Healthcare, and Environmental Science. One consistent focus is to evaluate the process of data for a given organization and identify opportunities to streamline data processing, management, and analytics. The end goal is to ensure high quality data is accessible and easy to use in a timely manner."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Alex Antonison's Blog",
    "section": "💼 Get In Touch",
    "text": "💼 Get In Touch\nAre you looking for a Data Engineer to help scale your data solutions? I am currently looking for my next opportunity.\nEmail me at adantonison@gmail.com, or send me a message on  if you’d like to chat!"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Alex Antonison's Blog",
    "section": "📮 Blog Posts",
    "text": "📮 Blog Posts\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n4/24/23\n\n\nThe Process of Data\n\n\nAlex Antonison\n\n\n\n\n3/1/23\n\n\nTool Selection Framework\n\n\nAlex Antonison\n\n\n\n\n11/22/22\n\n\nImpactful Data > Big Data\n\n\nAlex Antonison\n\n\n\n\n2/6/21\n\n\nvenv is a Data Professional’s Best Friend\n\n\nAlex Antonison\n\n\n\n\n10/4/19\n\n\nWhat are the different roles in data?\n\n\nAlex Antonison\n\n\n\n\n3/9/19\n\n\nEthical use of algorithms with data\n\n\nAlex Antonison\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Overview\nThis is a list of resources that I keep in mind when building data platforms. I also include some day-to-day tools I use to help manage projects, time, and communication.\n\n\nDevelopment\nDevelopment tools I use when building data solutions.\n\n\n\n\n\n\n\nDevelopment Tools\nDescription\n\n\n\n\nVisual Studio Code\nA code editor that supports a wide variety of languages and uses-cases with extensions\n\n\nGoogle Colab\nHosted jupyter notebooks for running python. Has access to GPU/TPU if you want to explore ML.\n\n\ntmux\nA terminal utility to allows running command line commands server-side.\n\n\nDocker\nPackage software code with a complete environment.\n\n\nServerless\nUtility that streamlines deploying serverless applications in cloud platforms such as AWS.\n\n\nAWS Python CDK\nPython Cloud Development Kit (CDK) allows for deploying AWS infrastructure using Python code.\n\n\ndata build tool(dbt)\nUtility that supports for creating ELT data pipelines run within a database. Used for normalizing data into a data warehouse and curating data marts. Supports data testing. Auto-generated documentation allows for making it easy to understand how data is flowing through the data platform.\n\n\nGitHub\nSource control platform of choice.\n\n\nGitHub Actions\nSupports Continuous Integration/Continuous Deployment (CI/CD)\n\n\ndbeaver\nTool for interacting with SQL databases\n\n\npre-commit\npre-commit is a tool that can run a series of checks when committing files to a git based source control\n\n\n\n\n\nProgramming Languages\nProgramming languages I consider when building data platforms, doing statistical analysis, machine learning, etc.\n\n\n\n\n\n\n\nLanguage\nPurpose\n\n\n\n\nPython\nA general usage language that supports building data platforms, data analytics, statistical analysis, and machine learning.\n\n\nR\nA language built for performing statistical analysis, machine learning, and data analysis.\n\n\nJulia\nA high performant language for scientific computing.\n\n\nScala\nFor writing performant Spark data applications.\n\n\nGo\nBuilding efficient data microservices.\n\n\nSQL\nStructured query language for pulling data from relational databases.\n\n\nbash\nA shell scripting language that can be helpful in automating tasks.\n\n\n\n\n\nPython Development Tools\n\n\n\n\n\n\n\n\nTool\nType\nPurpose\n\n\n\n\nblack\nClean Code\nblack is a python code formatter that helps keep code consistently formatted across teams\n\n\nisort\nClean Code\nisort helps organize python imports\n\n\nflake8\nClean Code\nflake8 is a code linter that enforces coding styles such as flagging unused imports\n\n\nmypy\nClean Code\nmypy is a static type checker that helps solve the problem in python with dynamic typing\n\n\npytest\nTesting\npytest is my unit testing tool of choice for it’s extensibility for building test harnesses\n\n\ngreat-expectations\nTesting\nSupports data quality testing and validation\n\n\nTest-Driven Data Analysis\nTesting\nA package that supports integrating data quality tests with unit tests such as pytest\n\n\npoetry\nEnvironment Management\nA python package that supports managing virtual environments\n\n\ndata version control(dvc)\nML Ops\nUtility that provides a streamlined approach for managing machine learning models from development to production.\n\n\nawswrangler\nAWS SDK\nAWS Development\n\n\n\n\n\nFavorite Python Analysis Tools\n\n\n\n\n\n\n\n\nTool\nType\nPurpose\n\n\n\n\njupyter notebooks\nAnalysis Tool\nJupyter notebooks allow for running python in a cell format and being able to immediately see the results\n\n\nPySpark\nData Processing\nProvides a Python interface to creating Spark applications\n\n\npandas\nDataframe\npandas allows for importing data into a tabular data structure and can perform cleaning and analysis activities in python\n\n\nscipy\nCalculations\nPython package for scientific computing\n\n\nnumpy\nCalculations\nPython package for mathematical computing\n\n\nstreamlit\nInteractive Data Visualization\nStreamlined method for creating interactive dashboards in python\n\n\nplotly\nInteractive Data Visualization\nStreamlined method for creating interactive visualizations in python\n\n\nplotnine\nStatic Data Visualization\nAllows the creation of ggplot visualizations in python\n\n\nscikit-learn\nMachine Learning\nMachine learning in python\n\n\nkeras\nDeep Learning\nA high level deep learning library for creating deep learning models\n\n\n\n\n\nR Development Tools\n\n\n\n\n\n\n\n\nTool\nType\nPurpose\n\n\n\n\nrenv\nEnvironment Management\nA tool for managing R packages used for a project.\n\n\nRStudio\nIDE\nA IDE tailored to running R code. It can also run Python but I have not tested this out.\n\n\n\n\n\nR Analysis Tools\n\n\n\n\n\n\n\n\nTool\nType\nPurpose\n\n\n\n\ntidyverse\nAnalysis\nAn ecosystem of r-packages that support doing a wide variety of data science/analytics tasks\n\n\nr-shiny\nInteractive Dashboard\nA framework for creating interactive dashboard sites with R.\n\n\n\n\n\nIT Tools\nThis section includes a wide variety of tools and platforms used to help build and deploy data platforms.\n\n\n\n\n\n\n\nItem\nPurpose\n\n\n\n\nnomachine\nTool for remotely connecting to Linux desktop.\n\n\nVMWare Workstation\nVirtual Machine for running a Windows VM on a Linux computer.\n\n\nAmazon Web Services\nCloud computing platform of choice for building and deploy data solutions.\n\n\nQuarto\nUsed for blog website.\n\n\nGitHub Pages\nFor serving up personal blog site.\n\n\nGoogle Domains\nUsed for custom domains\n\n\nSquarespace\nWebsite builder for managing business website.\n\n\ndraw.io\nFree tool for creating ERD diagrams, process flows, and AWS Architectures\n\n\nJuicebox Analytics\nData Storytelling tool\n\n\nMarkdown\nMarkup language for writing documentation.\n\n\noh-my-bash\nFramework for managing bash configuration. There is also a zshell equivalent, oh-my-zsh equivalent.\n\n\nCookiecutter\nA project templating tool.\n\n\n\n\n\nVisual Studio Code Extensions\nVisual Studio Code Extensions I find useful.\n\n\n\n\n\n\nVisual Studio Code Extension\n\n\n\n\nGitHub Pull Requests and Issues\n\n\nPython\n\n\nQuarto\n\n\nmarkdownlint\n\n\nProject Manager\n\n\nRemote Development\n\n\nCode Spell Checker\n\n\nDocker\n\n\nEdit csv\n\n\n\n\n\nProductivity and General Use\nProductivity tools and general use programs.\n\n\n\nItem\nPurpose\n\n\n\n\nTrello\nProductivity tool used for planning out work\n\n\nGoogle Workspace\nFrequently use Google Docs and Google Sheets\n\n\nBrave\nA privacy first browser built on Chrome\n\n\nFirefox\nFirefox has a Multi-Account Container supports logging into multiple accounts in a single Window.\n\n\nCalendly\nWebsite to streamline scheduling meetings.\n\n\nFeedly\nRSS Feed Tool that has a good free tier.\n\n\nzoom\nVideo Conferencing tool of choice."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Date and Location\nTalk Link\nAbstract\n\n\n\n\n2022-11-16\nSETAC North America Annual Meeting\nThe multi-sensor data system (MSDS): The strategy and execution of a user-friendly system that puts water quality data, from a variety of different manufacturers, in the hands of scientists\nWater quality data is vital to assess both risk and remedy effectiveness in or near water bodies. Although individual manufacturers of water quality devices typically have soft that allows for data downloading and visualizations, the integration of data from different sensors is left to the technical scientist(s). To address this problem, we created the Multi-Sensor Data System (MSDS). This system is designed to automate the ingestion of varying file types into a data lake, process that data into warehouses and marts, and serve up the integrated results via interactive dashboards. Included in this system is the ability for user-defined quality assurance filters, with update in real-time. The processing time need from raw data file upload to dashboard availability is approximately two minutes. This system was built using Amazon Web Services serverless technologies with a focus on low cost and maintenance, while maintaining fast response times and high reliability.\n\n\n2022-09-28\nNashville Analytics Summit\nStrategy and execution of a multi-sensor data system\nRecent advances in technology have led to the collection of data at speeds and volumes exponentially greater than ever before. In parallel, data management tools have been developed which allows those that possess these tools to more easily gather, process and interpret data. In this presentation, the MTSU Data Science Institute will demonstrate the concept and execution of their Multi-Sensor Data System (MSDS) using a water quality dataset to demonstrate functionality. This system was designed to automate the ingestion of varying files into a data lake, process into a data warehouse and marts, and serve up the results via a dashboard, in minutes. This system was built using AWS serverless technologies with a focus on low cost and maintenance, with high performance and reliability.\n\n\n2022-04-12\nMTSU Computational and Data Science Seminar\nStrategy and execution of a multi-sensor data system\nRecent advances in technology have led to the collection of data at speeds and volumes exponentially greater than ever before. In parallel, data management tools have been developed alongside this expansion which allows those that possess these tools to more easily gather, access, process, and interpret data. In this presentation, the MTSU Data Science Institute will demonstrate the concept and execution of their Multi-Sensor Data System (MSDS) using an applied water quality dataset from an ongoing project to demonstrate its functionality. This system was designed to automate the ingestion of multiple different data files and serve up the results via an interactive dashboard, in minutes. This cloud platform was built using Amazon Web Services serverless cloud technology with an emphasis on low cost and maintenance, with high performance and reliability.\n\n\n2021-11-17\nNashville Data Nerds\nA Nerdy Dive into Data Tools\nThere are a wide variety of ways to approach processing and analyzing data. This month, Alex Antonison will lead a conversation about the tools and programming languages he uses to improve his productivity when building data and analytic solutions. At the end he will review the results of the Data Nerds Tooling survey with the group to see what tools come out on top!"
  }
]