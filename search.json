[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an ex-Healthcare Data Engineer that has transitioned to working in Environmental Science. My approach is to take common techniques used in managing and processing Healthcare data to help streamline data in the Environmental Science space. I do this by building use-case specific data platforms that make Environmental Science data accessible and easy to use.\nI currently work with the Data Science Institute at MTSU and I have my own consulting business, Antonison Consulting Group."
  },
  {
    "objectID": "posts/different-roles-in-data/index.html",
    "href": "posts/different-roles-in-data/index.html",
    "title": "What are the different roles in data?",
    "section": "",
    "text": "Preface\nThe intended audience for this post are people interested in getting into data or businesses looking to use data to drive business value. With this in mind, I attempted to stay high level and provide enough information and key words for someone to find more in-depth resources on any given topic. Additionally, when highlighting skills, I focus on what I have observed being commonly used in industry based on experience, combing through job postings, and networking.\n\n\nWorking in data\nIn my experience, working in data can at times be a bit confusing and overwhelming because of the sheer breadth of skills and techniques that are needed to process, manage, and analyze data. In this post, I am going to group these sets of skills into different roles, highlight the order in which they should be built out in an organization, and emphasize the importance of only focusing on one role at a time. As someone who has done work in each of these areas, my effectiveness was significantly reduced when I had to wear multiple hats.The four roles I will be focusing on in this post are Data Analyst, Data Engineer, Data Scientist, and Machine Learning Engineer. While some of these roles can be broken down to be more specific, my goal of this blog post is to stay high level and emphasize broad concepts around each role. Additionally, there are other important roles such as Data Governance and Privacy and Data Curation which I will not be covering as this post is meant to be an introduction to a data team in an organization.\nAnd one last thing, although I do not advise trying to perform multiple roles at once, I think it can be beneficial if given the opportunity to try different roles and see what you enjoy most.\n\n\nThe different roles\n\n\nThe Data Analyst - Describes Data\nIn my opinion, the Data Analyst is at the heart of a data team. Their focus should be to describe data to help drive business value.\nA Data Analyst should be the first role a company should hire and the expectations should be for them to come in and work with stakeholders and leadership to understand a company’s goals and business problems and how data can help drive value. Once a company’s goals and business problems have been clearly defined, the Data Analyst should seek out data, wrangle it to make it tidy (see Tidy Data by Hadley Wickham), and then deliver reports and/or dashboards to communicate the results. The Data Analyst is what I consider more of an operational role that helps a business measure key indicators to help understand the state of their business as well as make data driven decisions.\nKey areas: data wrangling - data exploration - business intelligence - building dashboards (Key Performance Indicators (KPI), Operational) using tools like Tableau, PowerBI, Looker - building reports using tools like SQL Server Reporting Services (SSRS) - SQL - (optional but recommended) R or Python\n\n\nThe Data Engineer - Scales Data\nThe Data Engineer’s role is to scale an organization’s data ingestion and management.\nOnce a company has a firm understanding of their goals and business problems where data can help drive value, a Data Engineer can come in scale and build upon existing data processes. By scale, I mean the 3Vs of data(volume, variety, and velocity) and the ability to build automated data pipelines capable of bringing in data at the frequency a business needs to support applications, reports, dashboards, and analyses. Not only should a Data Engineer focus on Extract, transform, load (ETL) or Extract, load, transform (ELT), they should also have a good understanding of how to build optimized data warehouses/marts/stores to be useful for end users. Lastly, a Data Engineer should understand the concept of building production code, and by that I mean it needs to work consistently and reliably. In the event of an inevitable failure, it needs to fail reliably and have adequate logging so the issue can be debugged and resolved as efficiently as possible.\nKey Areas: concept of production code - optimized data warehousing/mart/store - Extract, transform, load (ETL) or Extract, load, transform (ELT) - data pipelines - automation - streaming data - web scraping - tools like Talend or Informatica - SQL - Python - docker\n—————————————————————\nMost companies can be highly successful doing Data Analytics and Engineering well. Without these two areas in place, a Data Scientist will be ineffective since they will simply be doing a combination of Data Analytics/Engineering. Often times this leads to mismatched skill sets and expectations on both the side of the organization as well as the Data Scientist.\n—————————————————————\n\n\nThe Data Scientist - Models Data\nThe role of the Data Scientist is to use machine learning and statistical techniques to develop models of data capable of predictions or prescriptions.\nThese models should have a focus on enhancing a business’ existing processes or develop new products that are not as feasible without machine learning techniques. This is more of a strategic role where often times the goal is to build upon the existing Data Analytic work. Similar to that of a Data Analyst, a Data Scientist will need to do data exploration and visualization to gain a better understanding of the data. Additionally, they will need to interact with business stakeholders to gain a firm understanding of company goals and business problems to help guide what models can be built that can drive value.\nKey Areas: feature engineering - statistical analysis - building / tuning / evaluating machine learning models - natural language processing - computer vision - R or Python - SQL\n\n\nThe Machine Learning Engineer - Scale Models\nThe role of the Machine Learning Engineer is to scale models by deploying and managing them in a production environment.\nA Machine Learning Engineer should work closely with the Data Science, Data Engineering, and Software Development teams to understand how a model needs to be deployed and build out necessary tools, APIs, or batch processes. Additionally, a Machine Learning Engineer needs to think about putting in place tools that will evaluate model performance and look for model drift to evaluate if a model needs to be retrained (great blog about this topic The Ultimate Guide to Model Retraining). Like a Data Engineer, they need to have a concept of production code since once a model is deployed, it is often their responsibility to ensure predictions are not only being returned but that they meet the necessary expectations.\nKey Areas: model drift - model retraining - the concept of production code - evaluation of models in production - machine learning - feature engineering - Python - SQL - docker - automation\n\n\n\nOverall Suggestions when working in Data\n\nFocus on the business problem - When starting data projects, it is easy to get distracted by all of the different available technologies. However, it is important to first focus on solving the business problem at hand versus using current “state of the art” solutions.\nBe intentional with technology selection - Whenever introducing a new piece of technology or programming language to a company, it is crucial to consider the consequences. Adding new technologies increases the overall complexity of a company’s technology stack as well as often times reducing collaboration and consumption of work across teams.\nSource Control - Regardless of role, I highly recommend all code (including SQL) is managed in a source control platform. By managing code through a source control platform, you can ensure code is accessible and changes to it over time can be tracked and managed. Depending on your team’s maturity and needs, I suggest checking out either Trunk Based or a Gitflow Development Strategy.\n\nSome popular source control platforms are GitHub, GitLab, and BitBucket\nIn instances where Data Analysts or Data Scientists are doing analyses or ad-hoc requests, I recommend having a single repository where the code for these requests are stored and managed with loose processes around merging. This will help ensure the code is located in a more discoverable and central location. With that, a more restrictive repository(s) should be in place that requires reviews for managing code related to specific project work.\n\nCloud computing is becoming more common place when working with data in organizations. AWS, Microsoft Azure, and Google all offer free tiers that can allow you to get some hands on experience even if your organization does not use cloud computing.\nPackage management - With respect to R and Python, I suggest looking into utilities to manage packages to help with reproducibility.\n\nFor R you I suggest using renv over packrat as it is much more efficient at managing packages.\nFor Python, I suggest using venv. It is simple and I have never had issues using it yet.\nIf you are new to Python, I also recommend checking out Anaconda. However, I will admit my experiences with using it to manage and share environments has not been great. I only recommend Anaconda as it allows you to more easily get started in Python.\n\nDocker - While managing packages is helpful, docker allows you to manage the entire environment of a script which ensures reproducibility. Dev Ops people will love you.\n\nA good resource for doing this in R is An Introduction to Docker for R Users by Colin Fay\nA good resource for Python is Docker\n\n\n\n\nUpdates\n\n2019-12-08\n\nrenv is now offered on cran, yay!\nChanged the method of suggest python environment management to being venv based on research and testing for consistently and reliably setting up and management environments in python."
  },
  {
    "objectID": "posts/ethical-use-algorithms-and-data/index.html",
    "href": "posts/ethical-use-algorithms-and-data/index.html",
    "title": "Ethical use of algorithms with data",
    "section": "",
    "text": "Preface\nBefore I talk about my views on ethics in the realm of Data Science, I first want to talk about how I got into Data Science. I spent the first two years of my career doing some analysis accompanied by mostly Data Engineering before I knew it was Data Engineering. At this two year mark in 2014, I was at a point where I wanted to figure out where I wanted to take my career next. After a bit of searching, I landed on the field of Data Science since it seemed like a perfect fit with my love of statistics and working with data. Like with anything, my first approach was to search and try and find as much information on the topic as possible. I took a few Coursera courses, followed top data scientists on twitter, read blogs, and listened to podcasts. More often than not, the topics were usually around applications of machine learning, interesting papers on machine learning models, or interviews with industry experts. However, periodically there was a post or podcast that would catch my eye on a topic like “gender biased word embeddings.” I quickly realized that although Data Science has the propensity for good, it also has a potential to harm individuals or entire communities. I began actively searching for these issues and came upon a couple of good articles that covered a wide variety of issues such as Biased Algorithms Are Everywhere, and No One Seems to Care\nHowever, before I go any further, I would like to introduce two concepts that are at the center of this post.\n\nMachine Learning: “Machine learning algorithms build a mathematical model of sample data, known as”training data”, in order to make predictions or decisions without being explicitly programmed to perform the task.” - https://en.wikipedia.org/wiki/Machine_learning\nAlgorithmic Bias: “Algorithmic bias occurs when a computer system reflects the implicit values of the humans who are involved in coding, collecting, selecting, or using data to train the algorithm.” - https://en.wikipedia.org/wiki/Algorithmic_bias\n\nWith that covered, I will move on to discuss a handful of cases where models are either biased, unethically created, or unethically used.\n\n\nGender biased word embeddings\nBefore I talk about how a word embedding can be gender biased, first I would like to discuss what is a word embedding. A word embedding is a useful natural language processing tool where a model can represent the relationships between words as mathematical values to allow for associations such as “man:king” with “woman:queen” and “paris:france” with “tokyo:japan”. Cool, right? However, this bias has been extended to “man:programmer” with “woman:homemaker”. Not cool. A common model used is Word2Vec developed by Google back in 2013 trained on Google News texts. Unfortunately, because the data this model was trained on was gender biased, so are the results. But there is hope! Researchers have done work to both quantify the bias and come up with methods to “debias” the embeddings in Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. For more on this, I recommend How Vector Space Mathematics Reveals the Hidden Sexism in Language.\n\n\nMortgage loan interest rate bias\nI find this to be a more traditional instance where financial institutions set out to use “big data” with “machine learning” to find ways to infer interest rates based on geography or characteristics of applicants. This is referred to as “Algorithmic Strategic Pricing”. A result of this, based on a study done by UC Berkly, African Americans and Latino borrowers pay more on purchase and refinance loans than White and Asian ethnicity borrowers. “The lenders may not be specifically targeting minorities in their pricing schemes, but by profiling non-shopping applicants they end up targeting them” said Adair Morse.\nFor more information, you can check out Minority homebuyers face widespread statistical lending discrimination, study finds or the study itself Consumer-Lending Discrimination in the Era of FinTech\n\n\nImage recognition bias\nImage recognition has become an everyday utility in society with many big tech companies using it in their product offerings like Google, Microsoft, and most notably, Facebook. However, without any industry benchmarks to ensure that these facial recognition applications perform well on people of all races, genders, and age, there are instances where either the systems simply do not work or the systems are offensive.\nA more notable instance of a facial recognition system failing to work was discovered by Joy Buolamwini, who is an African American PhD student at MIT’s Center for Civic Media. At the time of her discovery, she was a Computer Science undergraduate at Georgia Tech. In her undergrad, she was working on a research project to teach a computer to play “peek-a-boo” and found out that although the system had no issues recognizing her lighter skinned roommates, it had difficulty working with her. Her solution to this was to wear a white Halloween mask which would then detect her as white. More on this can be found here Ghosts in the Machine\nThis issue is not limited to Joy Buolamwini’s research, but could also be seen in Microsoft’s Kinect for the X-box. Back in 2010, it was observed that Microsoft’s Kinect often times would not work on people with darker skin. Is Microsoft’s Kinect Racist?. However, additionally worth noting is Microsoft advocating for there to be more regulation around image recognition in their blog post Facial recognition technology: The need for public regulation and corporate responsibility.\n\n\nMicrosoft Tay twitter bot\nI like using Microsoft’s Tay twitter bot as an example regarding ethics since it is an instance where the researchers themselves weren’t being unethical, but failed to consider how their model could be interacted with and manipulated. To provide a brief summary, Microsoft’s Tay twitter bot was a research experiment conducted where they built an artificial intelligence twitter bot that was supposed to learn how to mimic the speech of a 19 year old American girl by interacting with people on twitter. However, what they failed to consider was a series of internet users deciding to bombard the twitter bot with hateful speech. The end result required Microsoft to turn the twitter bot off after 16 hours.\nThis is an instance where although the researchers themselves were not being unethical, they failed to take into consideration how their twitter bot could be manipulated. As we build products, it is important not only to think about what the purpose of the model is but how it could be used to harm other people.\n\n\nFacebook Cambridge Analytica scandal\nAn ethics blog post would be incomplete without mentioning the Facebook–Cambridge Analytica data scandal. To briefly summarize, this is an instance where an organization used a survey app through Facebook to collect information from users for supposedly academic purposes. However, through manipulating Facebook’s app design, they were also able to collect the information of not only the users that agreed to the survey, but all of the users’ friends information as well. Furthermore, instead of using this information for academic purposes, they used it for both the Ted Cruz and Donald Trump political campaigns.\nThe two main takeaways here were that Cambridge Analytica both collected people’s information without their consent and then used the information for purposes beyond the consent given. Needless to say, collecting people’s information without their consent is clearly unethical. However, even when collecting people’s personal information ethically , it is important that measures are taken to ensure their information is protected and not misused.\n\n\nWays to improve\nA few closing thoughts on ways the Data Science industry can improve:\n\nBuild teams of people from diverse backgrounds to ensure underrepresented communities are not negatively impacted by biased models.\nAudit algorithms AND the data sets used to train models.\nEncourage companies to provide more information to users and researchers to help them better understand potential pitfalls and biases that may exist in their tools.\n\n\n\nAdditional sources\nIf you are still interested in looking more into this topic, I highly recommend checking out the following:\n\nArticles\n\n“We’re in a diversity crisis”: cofounder of Black in AI on what’s poisoning algorithms in our lives\nMachine bias risk assessments in criminal sentencing\n\nPodcasts\n\nData Skeptic - Data Ethics\nLinear Digressions - Facial recognition, society, and the law\nLinear Digressions - When is open data too open?\n\nBooks\n\nWeapons of Math Destruction"
  },
  {
    "objectID": "posts/venv-is-a-data-professionals-best-friend/index.html",
    "href": "posts/venv-is-a-data-professionals-best-friend/index.html",
    "title": "venv is a Data Professional’s Best Friend",
    "section": "",
    "text": "Preface\nHave you ever tried to share a project with a colleague and they struggled to run it? Have you ever spent hours trying to get a machine learning library to work? If so, there are solutions out there that can help with this! In this post, I will talk about a utility, virtual environments, that can be used to manage specific packages.\n\n\nvenv/renv\nWhen starting up a project, the first step I take is to spin up a virtual environment. In python, my personal favorite is the virtual environment utility built directly into python, python venv. In R, I like to use the package renv supported by RStudio. I do this for the following reasons:\n\nClean slate: When installing packages in a virtual environment, you can either use the latest versions of packages without worrying about impacting other projects or choose to use specific versions of packages.\nCollaboration: If I am working on a project with others, I can share the virtual environment configuration files, and they can spin up a virtual environment and run my code with the same packages.\nReproducibility: When I set the project down and I want to pick it up at a later time, I can ensure I am using the same packages as when I first started the project.\n\nFor python, you can run:\ncd /path/to/project\n# creates the virtual environment\npython -m venv venv\n# activates the virtual environment in your terminal\nsource venv/bin/activate\nOnce the environment is created and activated, you can then install all of the packages you need for your project and save them to a requirements.txt file that should be included with your project when sharing.\npip freeze > requirements.txt\nFor R, you can run:\n# You must first install `renv`\nRscript -e \"install.packages('renv')\"\n\n# Once installed, you can then run\ncd /path/to/project\nRscript -e \"renv::init()\"\nOnce set up and activated, you can start working on your project and install packages as needed. When you are ready to snapshot your environment, you can run the following command:\nRscript -e \"renv::snapshot()\"\nThis will update the renv.lock file that should be used when sharing your code to allow setting up a new environment from it. This is most easily used when working with R in a project in RStudio.\n\n\nLimitations\nWhile virtual environments are great at managing packages needed to run code, it is limited to just that. There are other aspects of running a project such as application or operating system dependencies. To completely solve for reproducing the environment for running code, you can explore using docker, a utility that will allow for capturing your entire environment which includes the Operating System and supporting applications. There is a steeper learning curve to this, and I plan on dedicating entire post(s) to the topic of how docker is a more comprehensive solution to reproducibility.\n\n\nOther virtual environment tools\nFor python, pipenv and poetry are virtual environment utilities that allow for managing various versions of packages in a project. You can manage packages ranging from selecting a specific version of a package to letting any version of that package be installed. While these tools can be helpful in managing packages and package dependencies, they come with additional complexity and a learning curve."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "venv is a Data Professional’s Best Friend\n\n\n\n\n\n\n\nreproducibility\n\n\nvirtual environment\n\n\nrenv\n\n\nvenv\n\n\npoetry\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2021\n\n\nAlex Antonison\n\n\n\n\n\n\n  \n\n\n\n\nWhat are the different roles in data?\n\n\n\n\n\n\n\ndata scieince\n\n\ndata engineering\n\n\ndata analyst\n\n\nrole\n\n\ncareer\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2019\n\n\nAlex Antonison\n\n\n\n\n\n\n  \n\n\n\n\nEthical use of algorithms with data\n\n\n\n\n\n\n\nethics\n\n\ndata\n\n\nalgorithms\n\n\nmachine learning\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2019\n\n\nAlex Antonison\n\n\n\n\n\n\nNo matching items"
  }
]