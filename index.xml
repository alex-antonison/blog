<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alex Antonison&#39;s Blog</title>
<link>https://www.alex-antonison.com/index.html</link>
<atom:link href="https://www.alex-antonison.com/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<image>
<url>https://www.alex-antonison.com/quarto.png</url>
<title>Alex Antonison&#39;s Blog</title>
<link>https://www.alex-antonison.com/index.html</link>
</image>
<generator>quarto-1.2.475</generator>
<lastBuildDate>Mon, 24 Apr 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>The Process of Data</title>
  <dc:creator>Alex Antonison</dc:creator>
  <link>https://www.alex-antonison.com/posts/the-process-of-data/index.html</link>
  <description><![CDATA[ 




<p>Do you have complex processes that you are struggling to streamline or automate? If so, my recommended approach has some key benefits:</p>
<p>‚≠ê Shorter timelines for improving an overall process</p>
<p>‚≠ê Faster ROI when investing in technology</p>
<p>‚≠ê Purpose built tools that are less expensive and easier to use</p>
<p>‚≠ê Improved data quality</p>
<p>‚≠ê Improved data accessibility and ease of use</p>
<p>üôà Problem: Building tools or platforms that streamline an entire or multiple parts of a process can be complex to build, complex to use, and expensive. This can also prevent you from being able to use existing purpose built tools since any one tool may only be able to streamline a portion of your process. üôà</p>
<p>üí° My recommended steps are: üí°</p>
<p>1Ô∏è‚É£ Document the whole process. This could include an overall process flow along with procedural steps for each part. This step is the most important since it is challenging to streamline an undocumented process.</p>
<p>2Ô∏è‚É£ Break the process down into separate parts. When a process is broken down into separate parts, it allows for less complex solutions to be used to automate it versus attempting to streamline an entire process with one tool.</p>
<p>3Ô∏è‚É£ Identify one part of the process that is the most important and/or time consuming. Rarely are all parts in a process made equal. Usually there are a handful of parts that are either crucial to success and/or time consuming.</p>
<p>4Ô∏è‚É£ Automate or Streamline Automate or streamline the part of the process identified in 3Ô∏è‚É£. This could involve using an existing tool or building a new tool. ‚ö† If a tool needs to be built, it is important to keep in mind this new tool is only meant to solve a single part of the overall process. ‚ö†</p>
<p>5Ô∏è‚É£ Iterate. Start back over at 1Ô∏è‚É£ Once you have automated or streamlined the most important and/or time consuming part of a process, it is helpful to go through the whole exercise again. This will help inform what worked, what didn‚Äôt work, and things to keep in mind when continuing to streamline a process.</p>
<p>Some final notes:</p>
<p>üóí Key people that do the process should be involved. There are always nuances to a process that can be missed in documentation.</p>
<p>üóí It is helpful to try and keep an overall technology architecture in mind. Selecting tools that can‚Äôt work with one another leads to data silos and forced manual parts of a process.</p>
<p>üóí Have a high level understanding of what an ideal end state/process looks like. This is helpful in thinking beyond the current process and can help inform technologies to use.</p>



 ]]></description>
  <guid>https://www.alex-antonison.com/posts/the-process-of-data/index.html</guid>
  <pubDate>Mon, 24 Apr 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>venv is a Data Professional‚Äôs Best Friend</title>
  <dc:creator>Alex Antonison</dc:creator>
  <link>https://www.alex-antonison.com/posts/venv-is-a-data-professionals-best-friend/index.html</link>
  <description><![CDATA[ 




<section id="preface" class="level3">
<h3 class="anchored" data-anchor-id="preface">Preface</h3>
<p>Have you ever tried to share a project with a colleague and they struggled to run it? Have you ever spent hours trying to get a machine learning library to work? If so, there are solutions out there that can help with this! In this post, I will talk about a utility, virtual environments, that can be used to manage specific packages.</p>
</section>
<section id="venvrenv" class="level3">
<h3 class="anchored" data-anchor-id="venvrenv">venv/renv</h3>
<p>When starting up a project, the first step I take is to spin up a virtual environment. In python, my personal favorite is the virtual environment utility built directly into python, <a href="https://docs.python.org/3/tutorial/venv.html">python venv</a>. In R, I like to use the package <a href="https://rstudio.github.io/renv/">renv</a> supported by RStudio. I do this for the following reasons:</p>
<ul>
<li>Clean slate: When installing packages in a virtual environment, you can either use the latest versions of packages without worrying about impacting other projects or choose to use specific versions of packages.</li>
<li>Collaboration: If I am working on a project with others, I can share the virtual environment configuration files, and they can spin up a virtual environment and run my code with the same packages.</li>
<li>Reproducibility: When I set the project down and I want to pick it up at a later time, I can ensure I am using the same packages as when I first started the project.</li>
</ul>
<p>For python, you can run:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="bu" style="color: null;">cd</span> /path/to/project</span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;"># creates the virtual environment</span></span>
<span id="cb1-3"><span class="ex" style="color: null;">python</span> <span class="at" style="color: #657422;">-m</span> venv venv</span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># activates the virtual environment in your terminal</span></span>
<span id="cb1-5"><span class="bu" style="color: null;">source</span> venv/bin/activate</span></code></pre></div>
<p>Once the environment is created and activated, you can then install all of the packages you need for your project and save them to a <code>requirements.txt</code> file that should be included with your project when sharing.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;">pip</span> freeze <span class="op" style="color: #5E5E5E;">&gt;</span> requirements.txt</span></code></pre></div>
<p>For R, you can run:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># You must first install `renv`</span></span>
<span id="cb3-2"><span class="ex" style="color: null;">Rscript</span> <span class="at" style="color: #657422;">-e</span> <span class="st" style="color: #20794D;">"install.packages('renv')"</span></span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;"># Once installed, you can then run</span></span>
<span id="cb3-5"><span class="bu" style="color: null;">cd</span> /path/to/project</span>
<span id="cb3-6"><span class="ex" style="color: null;">Rscript</span> <span class="at" style="color: #657422;">-e</span> <span class="st" style="color: #20794D;">"renv::init()"</span></span></code></pre></div>
<p>Once set up and activated, you can start working on your project and install packages as needed. When you are ready to snapshot your environment, you can run the following command:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;">Rscript</span> <span class="at" style="color: #657422;">-e</span> <span class="st" style="color: #20794D;">"renv::snapshot()"</span></span></code></pre></div>
<p>This will update the <code>renv.lock</code> file that should be used when sharing your code to allow setting up a new environment from it. This is most easily used when working with R in a project in RStudio.</p>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>While virtual environments are great at managing packages needed to run code, it is limited to just that. There are other aspects of running a project such as application or operating system dependencies. To completely solve for reproducing the environment for running code, you can explore using <a href="https://www.docker.com/">docker</a>, a utility that will allow for capturing your entire environment which includes the Operating System and supporting applications. There is a steeper learning curve to this, and I plan on dedicating entire post(s) to the topic of how docker is a more comprehensive solution to reproducibility.</p>
</section>
<section id="other-virtual-environment-tools" class="level3">
<h3 class="anchored" data-anchor-id="other-virtual-environment-tools">Other virtual environment tools</h3>
<p>For python, <a href="https://pipenv.pypa.io/en/latest/">pipenv</a> and <a href="https://python-poetry.org/">poetry</a> are virtual environment utilities that allow for managing various versions of packages in a project. You can manage packages ranging from selecting a specific version of a package to letting any version of that package be installed. While these tools can be helpful in managing packages and package dependencies, they come with additional complexity and a learning curve.</p>


</section>

 ]]></description>
  <guid>https://www.alex-antonison.com/posts/venv-is-a-data-professionals-best-friend/index.html</guid>
  <pubDate>Sat, 06 Feb 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>What are the different roles in data?</title>
  <dc:creator>Alex Antonison</dc:creator>
  <link>https://www.alex-antonison.com/posts/different-roles-in-data/index.html</link>
  <description><![CDATA[ 




<section id="preface" class="level3">
<h3 class="anchored" data-anchor-id="preface">Preface</h3>
<p>The intended audience for this post are people interested in getting into data or businesses looking to use data to drive business value. With this in mind, I attempted to stay high level and provide enough information and key words for someone to find more in-depth resources on any given topic. Additionally, when highlighting skills, I focus on what I have observed being commonly used in industry based on experience, combing through job postings, and networking.</p>
</section>
<section id="working-in-data" class="level3">
<h3 class="anchored" data-anchor-id="working-in-data">Working in data</h3>
<p>In my experience, working in data can at times be a bit confusing and overwhelming because of the sheer breadth of skills and techniques that are needed to process, manage, and analyze data. In this post, I am going to group these sets of skills into different roles, highlight the order in which they should be built out in an organization, and emphasize the importance of only focusing on one role at a time. As someone who has done work in each of these areas, my effectiveness was significantly reduced when I had to wear multiple hats.The four roles I will be focusing on in this post are Data Analyst, Data Engineer, Data Scientist, and Machine Learning Engineer. While some of these roles can be broken down to be more specific, my goal of this blog post is to stay high level and emphasize broad concepts around each role. Additionally, there are other important roles such as Data Governance and Privacy and Data Curation which I will not be covering as this post is meant to be an introduction to a data team in an organization.</p>
<p>And one last thing, although I do not advise trying to perform multiple roles at once, I think it can be beneficial if given the opportunity to try different roles and see what you enjoy most.</p>
</section>
<section id="the-different-roles" class="level3">
<h3 class="anchored" data-anchor-id="the-different-roles">The different roles</h3>
<p><img src="https://www.alex-antonison.com/posts/different-roles-in-data/data-organization-diagram.png" class="img-fluid"></p>
<section id="the-data-analyst---describes-data" class="level4">
<h4 class="anchored" data-anchor-id="the-data-analyst---describes-data"><strong>The Data Analyst - Describes Data</strong></h4>
<p>In my opinion, the Data Analyst is at the heart of a data team. Their focus should be to describe data to help drive business value.</p>
<p>A Data Analyst should be the first role a company should hire and the expectations should be for them to come in and work with stakeholders and leadership to understand a company‚Äôs goals and business problems and how data can help drive value. Once a company‚Äôs goals and business problems have been clearly defined, the Data Analyst should seek out data, wrangle it to make it <code>tidy</code> (see <a href="https://vita.had.co.nz/papers/tidy-data.pdf">Tidy Data</a> by Hadley Wickham), and then deliver reports and/or dashboards to communicate the results. The Data Analyst is what I consider more of an operational role that helps a business measure key indicators to help understand the state of their business as well as make data driven decisions.</p>
<p><em>Key areas:</em> data wrangling - data exploration - business intelligence - building dashboards (Key Performance Indicators (KPI), Operational) using tools like <a href="https://www.tableau.com/">Tableau</a>, <a href="https://powerbi.microsoft.com/en-us/">PowerBI</a>, <a href="https://looker.com/">Looker</a> - building reports using tools like SQL Server Reporting Services (<a href="https://docs.microsoft.com/en-us/sql/reporting-services/create-deploy-and-manage-mobile-and-paginated-reports?view=sql-server-2017">SSRS</a>) - SQL - (optional but recommended) R or Python</p>
</section>
<section id="the-data-engineer---scales-data" class="level4">
<h4 class="anchored" data-anchor-id="the-data-engineer---scales-data"><strong>The Data Engineer - Scales Data</strong></h4>
<p>The Data Engineer‚Äôs role is to scale an organization‚Äôs data ingestion and management.</p>
<p>Once a company has a firm understanding of their goals and business problems where data can help drive value, a Data Engineer can come in scale and build upon existing data processes. By scale, I mean the <a href="https://whatis.techtarget.com/definition/3Vs">3Vs of data</a>(volume, variety, and velocity) and the ability to build automated data pipelines capable of bringing in data at the frequency a business needs to support applications, reports, dashboards, and analyses. Not only should a Data Engineer focus on <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">Extract, transform, load (ETL)</a> or <a href="https://en.wikipedia.org/wiki/Extract,_load,_transform">Extract, load, transform (ELT)</a>, they should also have a good understanding of how to build optimized data warehouses/marts/stores to be useful for end users. Lastly, a Data Engineer should understand the concept of building production code, and by that I mean it needs to work consistently and reliably. In the event of an inevitable failure, it needs to fail reliably and have adequate logging so the issue can be debugged and resolved as efficiently as possible.</p>
<p><em>Key Areas:</em> concept of production code - optimized data warehousing/mart/store - <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">Extract, transform, load (ETL)</a> or <a href="https://en.wikipedia.org/wiki/Extract,_load,_transform">Extract, load, transform (ELT)</a> - data pipelines - automation - streaming data - web scraping - tools like <a href="https://www.talend.com/">Talend</a> or <a href="https://www.informatica.com">Informatica</a> - SQL - Python - docker</p>
<p><strong>‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî</strong></p>
<p><em>Most companies can be highly successful doing Data Analytics and Engineering well. Without these two areas in place, a Data Scientist will be ineffective since they will simply be doing a combination of Data Analytics/Engineering. Often times this leads to mismatched skill sets and expectations on both the side of the organization as well as the Data Scientist.</em></p>
<p><strong>‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî</strong></p>
</section>
<section id="the-data-scientist---models-data" class="level4">
<h4 class="anchored" data-anchor-id="the-data-scientist---models-data"><strong>The Data Scientist - Models Data</strong></h4>
<p>The role of the Data Scientist is to use machine learning and statistical techniques to develop models of data capable of predictions or prescriptions.</p>
<p>These models should have a focus on enhancing a business‚Äô existing processes or develop new products that are not as feasible without machine learning techniques. This is more of a strategic role where often times the goal is to build upon the existing Data Analytic work. Similar to that of a Data Analyst, a Data Scientist will need to do data exploration and visualization to gain a better understanding of the data. Additionally, they will need to interact with business stakeholders to gain a firm understanding of company goals and business problems to help guide what models can be built that can drive value.</p>
<p><em>Key Areas:</em> feature engineering - statistical analysis - building / tuning / evaluating machine learning models - natural language processing - computer vision - R or Python - SQL</p>
</section>
<section id="the-machine-learning-engineer---scale-models" class="level4">
<h4 class="anchored" data-anchor-id="the-machine-learning-engineer---scale-models"><strong>The Machine Learning Engineer - Scale Models</strong></h4>
<p>The role of the Machine Learning Engineer is to scale models by deploying and managing them in a production environment.</p>
<p>A Machine Learning Engineer should work closely with the Data Science, Data Engineering, and Software Development teams to understand how a model needs to be deployed and build out necessary tools, APIs, or batch processes. Additionally, a Machine Learning Engineer needs to think about putting in place tools that will evaluate model performance and look for model drift to evaluate if a model needs to be retrained (great blog about this topic <a href="https://mlinproduction.com/model-retraining/">The Ultimate Guide to Model Retraining</a>). Like a Data Engineer, they need to have a concept of production code since once a model is deployed, it is often their responsibility to ensure predictions are not only being returned but that they meet the necessary expectations.</p>
<p><em>Key Areas:</em> model drift - model retraining - the concept of production code - evaluation of models in production - machine learning - feature engineering - Python - SQL - docker - automation</p>
</section>
</section>
<section id="overall-suggestions-when-working-in-data" class="level3">
<h3 class="anchored" data-anchor-id="overall-suggestions-when-working-in-data">Overall Suggestions when working in Data</h3>
<ul>
<li><strong>Focus on the business problem</strong> - When starting data projects, it is easy to get distracted by all of the different available technologies. However, it is important to first focus on solving the business problem at hand versus using current ‚Äústate of the art‚Äù solutions.</li>
<li><strong>Be intentional with technology selection</strong> - Whenever introducing a new piece of technology or programming language to a company, it is crucial to consider the consequences. Adding new technologies increases the overall complexity of a company‚Äôs technology stack as well as often times reducing collaboration and consumption of work across teams.</li>
<li><strong>Source Control</strong> - Regardless of role, I highly recommend all code (including SQL) is managed in a source control platform. By managing code through a source control platform, you can ensure code is accessible and changes to it over time can be tracked and managed. Depending on your team‚Äôs maturity and needs, I suggest checking out either <a href="https://trunkbaseddevelopment.com/">Trunk Based</a> or a <a href="https://nvie.com/posts/a-successful-git-branching-model/">Gitflow</a> Development Strategy.
<ul>
<li>Some popular source control platforms are <a href="https://github.com/">GitHub</a>, <a href="https://about.gitlab.com/">GitLab</a>, and <a href="https://bitbucket.org/product">BitBucket</a></li>
<li>In instances where Data Analysts or Data Scientists are doing analyses or ad-hoc requests, I recommend having a single repository where the code for these requests are stored and managed with loose processes around merging. This will help ensure the code is located in a more discoverable and central location. With that, a more restrictive repository(s) should be in place that requires reviews for managing code related to specific project work.</li>
</ul></li>
<li><strong>Cloud computing</strong> is becoming more common place when working with data in organizations. AWS, Microsoft Azure, and Google all offer free tiers that can allow you to get some hands on experience even if your organization does not use cloud computing.</li>
<li><strong>Package management</strong> - With respect to R and Python, I suggest looking into utilities to manage packages to help with reproducibility.
<ul>
<li>For R you I suggest using <a href="https://github.com/rstudio/renv">renv</a> over packrat as it is much more efficient at managing packages.</li>
<li>For Python, I suggest using <a href="https://docs.python.org/3/library/venv.html">venv</a>. It is simple and I have never had issues using it yet.</li>
<li>If you are new to Python, I also recommend checking out <a href="https://www.anaconda.com/distribution/">Anaconda</a>. However, I will admit my experiences with using it to manage and share environments has not been great. I only recommend Anaconda as it allows you to more easily get started in Python.</li>
</ul></li>
<li><strong>Docker</strong> - While managing packages is helpful, docker allows you to manage the entire environment of a script which ensures reproducibility. <em>Dev Ops people will love you.</em>
<ul>
<li>A good resource for doing this in R is <a href="https://colinfay.me/docker-r-reproducibility/">An Introduction to Docker for R Users</a> by Colin Fay</li>
<li>A good resource for Python is <a href="https://www.fullstackpython.com/docker.html">Docker</a></li>
</ul></li>
</ul>
</section>
<section id="updates" class="level3">
<h3 class="anchored" data-anchor-id="updates">Updates</h3>
<section id="section" class="level4">
<h4 class="anchored" data-anchor-id="section">2019-12-08</h4>
<ul>
<li>renv is now offered on cran, yay!</li>
<li>Changed the method of suggest python environment management to being venv based on research and testing for consistently and reliably setting up and management environments in python.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://www.alex-antonison.com/posts/different-roles-in-data/index.html</guid>
  <pubDate>Fri, 04 Oct 2019 00:00:00 GMT</pubDate>
  <media:content url="https://www.alex-antonison.com/posts/different-roles-in-data/data-organization-diagram.png" medium="image" type="image/png" height="46" width="144"/>
</item>
<item>
  <title>Ethical use of algorithms with data</title>
  <dc:creator>Alex Antonison</dc:creator>
  <link>https://www.alex-antonison.com/posts/ethical-use-algorithms-and-data/index.html</link>
  <description><![CDATA[ 




<section id="preface" class="level3">
<h3 class="anchored" data-anchor-id="preface">Preface</h3>
<p>Before I talk about my views on ethics in the realm of Data Science, I first want to talk about how I got into Data Science. I spent the first two years of my career doing some analysis accompanied by mostly Data Engineering before I knew it was Data Engineering. At this two year mark in 2014, I was at a point where I wanted to figure out where I wanted to take my career next. After a bit of searching, I landed on the field of Data Science since it seemed like a perfect fit with my love of statistics and working with data. Like with anything, my first approach was to search and try and find as much information on the topic as possible. I took a few Coursera courses, followed <em>top data scientists on twitter</em>, read blogs, and listened to podcasts. More often than not, the topics were usually around applications of machine learning, interesting papers on machine learning models, or interviews with industry experts. However, periodically there was a post or podcast that would catch my eye on a topic like ‚Äúgender biased word embeddings.‚Äù I quickly realized that although Data Science has the propensity for good, it also has a potential to harm individuals or entire communities. I began actively searching for these issues and came upon a couple of good articles that covered a wide variety of issues such as <a href="https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/">Biased Algorithms Are Everywhere, and No One Seems to Care</a></p>
<p>However, before I go any further, I would like to introduce two concepts that are at the center of this post.</p>
<ul>
<li><strong>Machine Learning:</strong> ‚ÄúMachine learning algorithms build a mathematical model of sample data, known as‚Äùtraining data‚Äù, in order to make predictions or decisions without being explicitly programmed to perform the task.‚Äù - <a href="https://en.wikipedia.org/wiki/Machine_learning">https://en.wikipedia.org/wiki/Machine_learning</a></li>
<li><strong>Algorithmic Bias:</strong> ‚ÄúAlgorithmic bias occurs when a computer system reflects the implicit values of the humans who are involved in coding, collecting, selecting, or using data to train the algorithm.‚Äù - <a href="https://en.wikipedia.org/wiki/Algorithmic_bias">https://en.wikipedia.org/wiki/Algorithmic_bias</a></li>
</ul>
<p>With that covered, I will move on to discuss a handful of cases where models are either biased, unethically created, or unethically used.</p>
</section>
<section id="gender-biased-word-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="gender-biased-word-embeddings">Gender biased word embeddings</h3>
<p>Before I talk about how a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> can be gender biased, first I would like to discuss what is a word embedding. A word embedding is a useful natural language processing tool where a model can represent the relationships between words as mathematical values to allow for associations such as ‚Äúman:king‚Äù with ‚Äúwoman:queen‚Äù and ‚Äúparis:france‚Äù with ‚Äútokyo:japan‚Äù. Cool, right? However, this bias has been extended to ‚Äúman:programmer‚Äù with ‚Äúwoman:homemaker‚Äù. Not cool. A common model used is Word2Vec developed by Google back in 2013 trained on Google News texts. Unfortunately, because the data this model was trained on was gender biased, so are the results. But there is hope! Researchers have done work to both quantify the bias and come up with methods to ‚Äúdebias‚Äù the embeddings in <a href="https://arxiv.org/abs/1607.06520">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a>. For more on this, I recommend <a href="https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/">How Vector Space Mathematics Reveals the Hidden Sexism in Language</a>.</p>
</section>
<section id="mortgage-loan-interest-rate-bias" class="level3">
<h3 class="anchored" data-anchor-id="mortgage-loan-interest-rate-bias">Mortgage loan interest rate bias</h3>
<p>I find this to be a more traditional instance where financial institutions set out to use ‚Äúbig data‚Äù with ‚Äúmachine learning‚Äù to find ways to infer interest rates based on geography or characteristics of applicants. This is referred to as ‚ÄúAlgorithmic Strategic Pricing‚Äù. A result of this, <a href="http://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf">based on a study done by UC Berkly</a>, African Americans and Latino borrowers pay more on purchase and refinance loans than White and Asian ethnicity borrowers. ‚ÄúThe lenders may not be specifically targeting minorities in their pricing schemes, but by profiling non-shopping applicants they end up targeting them‚Äù said <a href="http://faculty.haas.berkeley.edu/morse/">Adair Morse</a>.</p>
<p>For more information, you can check out <a href="http://newsroom.haas.berkeley.edu/minority-homebuyers-face-widespread-statistical-lending-discrimination-study-finds/">Minority homebuyers face widespread statistical lending discrimination, study finds</a> or the study itself <a href="http://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf">Consumer-Lending Discrimination in the Era of FinTech</a></p>
</section>
<section id="image-recognition-bias" class="level3">
<h3 class="anchored" data-anchor-id="image-recognition-bias">Image recognition bias</h3>
<p>Image recognition has become an everyday utility in society with many big tech companies using it in their product offerings like Google, Microsoft, and most notably, Facebook. However, without any industry benchmarks to ensure that these facial recognition applications perform well on people of all races, genders, and age, there are instances where either the systems simply do not work or the systems are offensive.</p>
<p>A more notable instance of a facial recognition system failing to work was discovered by Joy Buolamwini, who is an African American PhD student at MIT‚Äôs Center for Civic Media. At the time of her discovery, she was a Computer Science undergraduate at Georgia Tech. In her undergrad, she was working on a research project to teach a computer to play ‚Äúpeek-a-boo‚Äù and found out that although the system had no issues recognizing her lighter skinned roommates, it had difficulty working with her. Her solution to this was to wear a white Halloween mask which would then detect her as white. More on this can be found here <a href="https://www.pbs.org/wgbh/nova/article/ai-bias/">Ghosts in the Machine</a></p>
<p>This issue is not limited to Joy Buolamwini‚Äôs research, but could also be seen in Microsoft‚Äôs Kinect for the X-box. Back in 2010, it was observed that Microsoft‚Äôs Kinect often times would not work on people with darker skin. <a href="https://www.pcworld.com/article/209708/Is_Microsoft_Kinect_Racist.html">Is Microsoft‚Äôs Kinect Racist?</a>. However, additionally worth noting is Microsoft advocating for there to be more regulation around image recognition in their blog post <a href="https://blogs.microsoft.com/on-the-issues/2018/07/13/facial-recognition-technology-the-need-for-public-regulation-and-corporate-responsibility/">Facial recognition technology: The need for public regulation and corporate responsibility</a>.</p>
</section>
<section id="microsoft-tay-twitter-bot" class="level3">
<h3 class="anchored" data-anchor-id="microsoft-tay-twitter-bot">Microsoft Tay twitter bot</h3>
<p>I like using Microsoft‚Äôs Tay twitter bot as an example regarding ethics since it is an instance where the researchers themselves weren‚Äôt being unethical, but failed to consider how their model could be interacted with and manipulated. To provide a brief summary, Microsoft‚Äôs Tay twitter bot was a research experiment conducted where they built an artificial intelligence twitter bot that was supposed to learn how to mimic the speech of a 19 year old American girl by interacting with people on twitter. However, what they failed to consider was a series of internet users deciding to bombard the twitter bot with hateful speech. The end result required Microsoft to turn the twitter bot off after 16 hours.</p>
<p>This is an instance where although the researchers themselves were not being unethical, they failed to take into consideration how their twitter bot could be manipulated. As we build products, it is important not only to think about what the purpose of the model is but how it could be used to harm other people.</p>
</section>
<section id="facebook-cambridge-analytica-scandal" class="level3">
<h3 class="anchored" data-anchor-id="facebook-cambridge-analytica-scandal">Facebook Cambridge Analytica scandal</h3>
<p>An ethics blog post would be incomplete without mentioning the <a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal">Facebook‚ÄìCambridge Analytica data scandal</a>. To briefly summarize, this is an instance where an organization used a survey app through Facebook to collect information from users for supposedly academic purposes. However, through manipulating Facebook‚Äôs app design, they were also able to collect the information of not only the users that agreed to the survey, but all of the users‚Äô friends information as well. Furthermore, instead of using this information for academic purposes, they used it for both the Ted Cruz and Donald Trump political campaigns.</p>
<p>The two main takeaways here were that Cambridge Analytica both collected people‚Äôs information without their consent and then used the information for purposes beyond the consent given. Needless to say, collecting people‚Äôs information without their consent is clearly unethical. However, even when collecting people‚Äôs personal information ethically , it is important that measures are taken to ensure their information is protected and not misused.</p>
</section>
<section id="ways-to-improve" class="level3">
<h3 class="anchored" data-anchor-id="ways-to-improve">Ways to improve</h3>
<p>A few closing thoughts on ways the Data Science industry can improve:</p>
<ul>
<li>Build teams of people from diverse backgrounds to ensure underrepresented communities are not negatively impacted by biased models.</li>
<li>Audit algorithms AND the data sets used to train models.</li>
<li>Encourage companies to provide more information to users and researchers to help them better understand potential pitfalls and biases that may exist in their tools.</li>
</ul>
</section>
<section id="additional-sources" class="level3">
<h3 class="anchored" data-anchor-id="additional-sources">Additional sources</h3>
<p>If you are still interested in looking more into this topic, I highly recommend checking out the following:</p>
<ul>
<li>Articles
<ul>
<li><a href="https://www.technologyreview.com/s/610192/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/">‚ÄúWe‚Äôre in a diversity crisis‚Äù: cofounder of Black in AI on what‚Äôs poisoning algorithms in our lives</a></li>
<li><a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine bias risk assessments in criminal sentencing</a></li>
</ul></li>
<li>Podcasts
<ul>
<li><a href="https://dataskeptic.com/blog/episodes/2018/data-ethics">Data Skeptic - Data Ethics</a></li>
<li><a href="http://lineardigressions.com/episodes/2018/12/30/facial-recognition-society-and-you">Linear Digressions - Facial recognition, society, and the law</a></li>
<li><a href="http://lineardigressions.com/episodes/2018/2/25/when-is-open-data-too-open">Linear Digressions - When is open data too open?</a></li>
</ul></li>
<li>Books
<ul>
<li><a href="https://weaponsofmathdestructionbook.com/">Weapons of Math Destruction</a></li>
</ul></li>
</ul>


</section>

 ]]></description>
  <guid>https://www.alex-antonison.com/posts/ethical-use-algorithms-and-data/index.html</guid>
  <pubDate>Sat, 09 Mar 2019 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
